{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective - To try different CNN architecture ([3x3],[5x5],[7x7]) on MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepak\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "batch_size = 68\n",
    "num_classes = 10\n",
    "epochs = 4\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used draw Categorical Crossentropy Loss VS No. of epochs plot\n",
    "def plt_dynamic(x, vy, ty):\n",
    "  plt.figure(figsize=(10,5))\n",
    "  plt.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "  plt.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "  plt.xlabel('Epochs') \n",
    "  plt.ylabel('Categorical Crossentropy Loss')\n",
    "  plt.title('\\nCategorical Crossentropy Loss VS Epochs')\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: CNN with 3x3 Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               819456    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 914,698\n",
      "Trainable params: 914,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n"
     ]
    }
   ],
   "source": [
    "# Initialising the model\n",
    "model_3 = Sequential()\n",
    "\n",
    "# Adding first conv layer\n",
    "model_3.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "\n",
    "# Adding second conv layer\n",
    "model_3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Adding Maxpooling layer\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Adding Dropout\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "# Adding third conv layer\n",
    "model_3.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "# Adding Maxpooling layer\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Adding Dropout\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "# Adding flatten layer\n",
    "model_3.add(Flatten())\n",
    "\n",
    "# Adding first hidden layer\n",
    "model_3.add(Dense(256, activation='relu',kernel_initializer=he_normal(seed=None)))\n",
    "\n",
    "# Adding Dropout\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "# Adding output layer\n",
    "model_3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Printing model Summary\n",
    "print(model_3.summary())\n",
    "\n",
    "# Compiling the model\n",
    "model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the data to the model\n",
    "history_3 = model_3.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the model\n",
    "model_4 = Sequential()\n",
    "\n",
    "# Adding first conv layer\n",
    "model_4.add(Conv2D(8, kernel_size=(5, 5),padding='same',activation='relu',input_shape=input_shape))\n",
    "\n",
    "# Adding second conv layer\n",
    "model_4.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "\n",
    "# Adding Maxpooling layer\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "# Adding Dropout\n",
    "model_4.add(Dropout(0.25))\n",
    "\n",
    "# Adding third conv layer\n",
    "model_4.add(Conv2D(32, (5, 5),padding='same', activation='relu'))\n",
    "\n",
    "# Adding Maxpooling layer\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "# Adding Dropout\n",
    "model_4.add(Dropout(0.25))\n",
    "\n",
    "# Adding fourth conv layer\n",
    "model_4.add(Conv2D(64, (5, 5),padding='same',activation='relu'))\n",
    "\n",
    "# Adding Maxpooling layer\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "# Adding Dropout\n",
    "model_4.add(Dropout(0.25))\n",
    "\n",
    "# Adding flatten layer\n",
    "model_4.add(Flatten())\n",
    "\n",
    "# Adding first hidden layer\n",
    "model_4.add(Dense(256, activation='relu',kernel_initializer=he_normal(seed=None)))\n",
    "\n",
    "# Adding Batch Normalization\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "# Adding Dropout\n",
    "model_4.add(Dropout(0.5))\n",
    "\n",
    "# Adding output layer\n",
    "model_4.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Printing model Summary\n",
    "print(model_4.summary())\n",
    "\n",
    "# Compiling the model\n",
    "model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the data to the model\n",
    "history_4 = model_4.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "score = model_4.evaluate(x_test, y_test, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Test and train accuracy of the model\n",
    "model_4_test = score[1]\n",
    "model_4_train = max(history_4.history['acc'])\n",
    "\n",
    "# Plotting Train and Test Loss VS no. of epochs\n",
    "# list of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "# Validation loss\n",
    "vy = history_4.history['val_loss']\n",
    "# Training loss\n",
    "ty = history_4.history['loss']\n",
    "\n",
    "# Calling the function to draw the plot\n",
    "plt_dynamic(x, vy, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation function= Relu\n",
      "Epochs= 12\n",
      "Batch size= 128\n",
      "+--------+------------+---------------+-----------------------------+\n",
      "| Kernel | CNN layers | Test accuracy |        Optimal Epochs       |\n",
      "+--------+------------+---------------+-----------------------------+\n",
      "|  3x3   |  3 layers  |     0.9945    |              8              |\n",
      "|  4x4   |  4 layers  |     0.9930    |              12             |\n",
      "|  5x5   |  5 layers  |     0.9953    | Didn't merge till 12 epochs |\n",
      "+--------+------------+---------------+-----------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(\"Activation function= Relu\")\n",
    "print(\"Epochs= 12\")\n",
    "print(\"Batch size= 128\")\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "x=PrettyTable()\n",
    "x.field_names = [\"Kernel\",\"CNN layers\",\"Test accuracy\",\"Optimal Epochs\"]\n",
    "x.add_row([\"3x3\",\"3 layers\",\"0.9945\",\"8\"])\n",
    "x.add_row([\"4x4\",\"4 layers\",\"0.9930\",\"12\"])\n",
    "x.add_row([\"5x5\",\"5 layers\",\"0.9953\",\"Didn't merge till 12 epochs\"])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
